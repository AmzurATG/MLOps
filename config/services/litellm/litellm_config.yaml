# ============================================================================
# LiteLLM Local Development Configuration with MLflow Observability
# ============================================================================
# Purpose: Local testing configuration that logs to your existing MLflow server
# Date: December 31, 2025
# ============================================================================

# ============================================================================
# MODEL LIST
# ============================================================================
model_list:
  # --------------------------------------------------------------------------
  # OpenAI GPT-4 Models
  # --------------------------------------------------------------------------
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY
      rpm: 500
      tpm: 150000
    

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      rpm: 500
      tpm: 150000
   

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      rpm: 1000
      tpm: 200000
    

  # --------------------------------------------------------------------------
  # OpenAI GPT-3.5 Models
  # --------------------------------------------------------------------------
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
      rpm: 3000
      tpm: 500000
    

  # --------------------------------------------------------------------------
  # Embedding Models
  # --------------------------------------------------------------------------
  - model_name: text-embedding-3-large
    litellm_params:
      model: openai/text-embedding-3-large
      api_key: os.environ/OPENAI_API_KEY
      rpm: 5000
      tpm: 1000000
    

  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY
      rpm: 5000
      tpm: 1000000
    

  - model_name: text-embedding-ada-002
    litellm_params:
      model: openai/text-embedding-ada-002
      api_key: os.environ/OPENAI_API_KEY
      rpm: 5000
      tpm: 1000000


  # --------------------------------------------------------------------------
  # Groq Models (FREE TIER) - Verified Working January 2026
  # --------------------------------------------------------------------------
  - model_name: llama-3.1-8b
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY
      rpm: 30
      tpm: 6000

  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
      rpm: 30
      tpm: 6000

# ============================================================================
# LITELLM SETTINGS - WITH MLFLOW OBSERVABILITY
# ============================================================================
litellm_settings:
  # Drop unsupported parameters instead of erroring
  drop_params: true
  
  # Enable detailed logging for debugging
  set_verbose: true
  
  # *** MLflow Integration ***
  # All LLM calls automatically logged to MLflow
  success_callback: ["mlflow"]
  failure_callback: ["mlflow"]
  
  # Enable request/response caching (uses Redis)
  cache: true
  cache_params:
    type: redis
    ttl: 3600  # 1 hour cache
  
  # Routing settings
  num_retries: 2
  timeout: 300

# ============================================================================
# GENERAL SETTINGS
# ============================================================================
general_settings:
  # Master key for admin access
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Database for storing models, keys, users
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true
  
  # UI Configuration
  ui_username: os.environ/UI_USERNAME
  ui_password: os.environ/UI_PASSWORD
  
  # Logging
  detailed_debug: true
  
  # Cost tracking
  track_cost_per_deployment: true

# ============================================================================
# ROUTER SETTINGS
# ============================================================================
router_settings:
  routing_strategy: simple-shuffle
  enable_fallbacks: true
  num_retries: 2
  timeout: 300
  cooldown_time: 30

# ============================================================================
# NOTES
# ============================================================================
# This configuration:
# 1. Logs all LLM calls to your existing MLflow server at http://exp-mlflow:5000
# 2. Uses Redis for caching to reduce API costs
# 3. Stores configurations in PostgreSQL
# 4. Provides a UI at http://localhost:4000/ui
#
# MLflow Integration:
# - Every completion call → logged as a trace in MLflow
# - Includes: prompt, response, tokens, cost, latency
# - View in MLflow UI: http://localhost:15000 → Traces tab
#
# Testing:
# curl -X POST 'http://localhost:4000/chat/completions' \
#   -H 'Content-Type: application/json' \
#   -H 'Authorization: Bearer sk-local-dev-2025' \
#   -d '{
#     "model": "gpt-3.5-turbo",
#     "messages": [{"role": "user", "content": "Hello!"}]
#   }'
# ============================================================================