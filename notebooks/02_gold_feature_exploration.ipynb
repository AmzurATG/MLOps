{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Feature Exploration - Testing fraud_training_data Features\n",
    "\n",
    "**Purpose:** Explore features created by `mlops_training_features.py`\n",
    "\n",
    "**NOT part of workflow** - Just for exploration/validation\n",
    "\n",
    "Use this to:\n",
    "- Validate fraud_training_data table\n",
    "- Analyze feature distributions\n",
    "- Check feature quality\n",
    "- Test feature correlations with fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trino.dbapi import connect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to iceberg_dev.gold.fraud_training_data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TRINO_HOST = os.getenv('TRINO_HOST', 'trino')\n",
    "TRINO_PORT = int(os.getenv('TRINO_PORT', '8080'))\n",
    "ICEBERG_CATALOG = os.getenv('ICEBERG_CATALOG', 'iceberg_dev')\n",
    "\n",
    "# Connect to Trino\n",
    "conn = connect(\n",
    "    host=TRINO_HOST,\n",
    "    port=TRINO_PORT,\n",
    "    user='trino',\n",
    "    catalog=ICEBERG_CATALOG,\n",
    "    schema='gold'\n",
    ")\n",
    "\n",
    "TRAINING_TABLE = f'{ICEBERG_CATALOG}.gold.fraud_training_data'\n",
    "print(f\"âœ“ Connected to {TRAINING_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load training data (with chunking support for 1M+ scalability)\n# For large datasets, set NOTEBOOK_CHUNK_SIZE environment variable\n\nCHUNK_SIZE = int(os.getenv(\"NOTEBOOK_CHUNK_SIZE\", \"0\"))  # 0 = use LIMIT\n\nif CHUNK_SIZE > 0:\n    # Streaming mode for 1M+ records\n    print(f\"Loading training data in chunks of {CHUNK_SIZE}...\")\n    query = f\"SELECT * FROM {TRAINING_TABLE}\"\n    chunks = []\n    for chunk in pd.read_sql(query, conn, chunksize=CHUNK_SIZE):\n        chunks.append(chunk)\n        print(f\"  Loaded chunk: {len(chunk)} rows\")\n    df = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n    # Sort after loading (more efficient than ORDER BY on full table)\n    df = df.sort_values('event_timestamp', ascending=False)\nelse:\n    # Quick exploration mode with LIMIT\n    df = pd.read_sql(f\"SELECT * FROM {TRAINING_TABLE} ORDER BY event_timestamp DESC LIMIT 50000\", conn)\n\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Date range: {df['event_timestamp'].min()} to {df['event_timestamp'].max()}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature groups from mlops_training_features.py\nprint(\"\\nðŸ“‹ All Features:\")\nprint(f\"Total columns: {len(df.columns)}\\n\")\n\n# Note: customer_age not in fraud_training_data (only in source)\nprimitives = ['amount', 'quantity', 'country', 'device_type', 'payment_method', \n              'category', 'account_age_days', 'tx_hour', 'tx_dayofweek']\n\ncalendar = ['transaction_day', 'transaction_month', 'is_weekend', 'is_night']\n\naddress = ['shipping_country', 'billing_country', 'address_mismatch', 'high_risk_shipping']\n\nbehavioral = ['transactions_before', 'total_spend_before', 'avg_amount_before', \n              'customer_tenure_days', 'days_since_last_tx', 'is_new_customer', 'is_very_new_account']\n\nspending = ['total_spend_7d', 'tx_count_7d', 'total_spend_30d', 'tx_count_30d', \n            'avg_amount_30d', 'max_amount_30d', 'num_countries_90d', 'num_payment_methods_30d']\n\nrisk = ['amount_vs_avg', 'is_high_amount_vs_hist', 'high_velocity', \n        'risky_payment', 'risky_category', 'ip_prefix']\n\n# Label column (final_label = COALESCE(reviewed_label, is_fraudulent))\nLABEL_COL = 'final_label'\n\nprint(f\"Primitives ({len(primitives)}): {primitives}\")\nprint(f\"Calendar ({len(calendar)}): {calendar}\")\nprint(f\"Address ({len(address)}): {address}\")\nprint(f\"Behavioral ({len(behavioral)}): {behavioral}\")\nprint(f\"Spending ({len(spending)}): {spending}\")\nprint(f\"Risk ({len(risk)}): {risk}\")\nprint(f\"\\nLabel column: {LABEL_COL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fraud distribution\nprint(\"\\nðŸ“Š Fraud Distribution:\")\nprint(df[LABEL_COL].value_counts())\nprint(f\"Fraud rate: {df[LABEL_COL].mean():.2%}\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\ndf[LABEL_COL].value_counts().plot(kind='bar', ax=axes[0], title='Fraud Distribution')\naxes[0].set_xlabel('Is Fraudulent (final_label)')\naxes[0].set_ylabel('Count')\n\n# Fraud rate over time\ndf['event_timestamp'] = pd.to_datetime(df['event_timestamp'])\ndf.set_index('event_timestamp')[LABEL_COL].resample('D').mean().plot(ax=axes[1], title='Fraud Rate Over Time')\naxes[1].set_ylabel('Fraud Rate')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Behavioral Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Behavioral features by fraud label\nprint(\"\\nðŸ§‘ Behavioral Features by Fraud Label:\")\nprint(df.groupby(LABEL_COL)[behavioral].mean())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize behavioral features\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# transactions_before\ndf.groupby(LABEL_COL)['transactions_before'].hist(bins=30, alpha=0.7, ax=axes[0,0])\naxes[0,0].set_title('Transactions Before')\naxes[0,0].set_xlabel('Count')\naxes[0,0].legend(['Legit', 'Fraud'])\n\n# avg_amount_before\ndf.groupby(LABEL_COL)['avg_amount_before'].hist(bins=30, alpha=0.7, ax=axes[0,1])\naxes[0,1].set_title('Avg Amount Before')\naxes[0,1].set_xlabel('Amount')\n\n# customer_tenure_days\ndf.groupby(LABEL_COL)['customer_tenure_days'].hist(bins=30, alpha=0.7, ax=axes[0,2])\naxes[0,2].set_title('Customer Tenure Days')\naxes[0,2].set_xlabel('Days')\n\n# days_since_last_tx\ndf[df['days_since_last_tx'] < 100].groupby(LABEL_COL)['days_since_last_tx'].hist(bins=30, alpha=0.7, ax=axes[1,0])\naxes[1,0].set_title('Days Since Last TX')\naxes[1,0].set_xlabel('Days')\n\n# is_new_customer\ndf.groupby([LABEL_COL, 'is_new_customer']).size().unstack().plot(kind='bar', ax=axes[1,1])\naxes[1,1].set_title('New Customer')\naxes[1,1].set_xlabel('Is Fraudulent')\n\n# is_very_new_account\ndf.groupby([LABEL_COL, 'is_very_new_account']).size().unstack().plot(kind='bar', ax=axes[1,2])\naxes[1,2].set_title('Very New Account')\naxes[1,2].set_xlabel('Is Fraudulent')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spending Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Spending features by fraud label\nprint(\"\\nðŸ’° Spending Features by Fraud Label:\")\nprint(df.groupby(LABEL_COL)[spending].mean())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize spending features\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# total_spend_7d\ndf[df['total_spend_7d'] < 10000].groupby(LABEL_COL)['total_spend_7d'].hist(bins=30, alpha=0.7, ax=axes[0,0])\naxes[0,0].set_title('Total Spend 7D')\naxes[0,0].set_xlabel('Amount')\naxes[0,0].legend(['Legit', 'Fraud'])\n\n# total_spend_30d\ndf[df['total_spend_30d'] < 20000].groupby(LABEL_COL)['total_spend_30d'].hist(bins=30, alpha=0.7, ax=axes[0,1])\naxes[0,1].set_title('Total Spend 30D')\naxes[0,1].set_xlabel('Amount')\n\n# tx_count_7d\ndf.groupby(LABEL_COL)['tx_count_7d'].hist(bins=30, alpha=0.7, ax=axes[0,2])\naxes[0,2].set_title('TX Count 7D')\naxes[0,2].set_xlabel('Count')\n\n# tx_count_30d\ndf.groupby(LABEL_COL)['tx_count_30d'].hist(bins=30, alpha=0.7, ax=axes[1,0])\naxes[1,0].set_title('TX Count 30D')\naxes[1,0].set_xlabel('Count')\n\n# num_countries_90d\ndf.groupby(LABEL_COL)['num_countries_90d'].hist(bins=20, alpha=0.7, ax=axes[1,1])\naxes[1,1].set_title('Countries 90D')\naxes[1,1].set_xlabel('Count')\n\n# num_payment_methods_30d\ndf.groupby(LABEL_COL)['num_payment_methods_30d'].hist(bins=10, alpha=0.7, ax=axes[1,2])\naxes[1,2].set_title('Payment Methods 30D')\naxes[1,2].set_xlabel('Count')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Risk Indicator Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Risk indicators by fraud label\nprint(\"\\nðŸš¨ Risk Indicators by Fraud Label:\")\nrisk_numeric = ['amount_vs_avg', 'is_high_amount_vs_hist', 'high_velocity', 'risky_payment', 'risky_category']\nprint(df.groupby(LABEL_COL)[risk_numeric].mean())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize risk indicators\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# amount_vs_avg (capped for visualization)\ndf[df['amount_vs_avg'] < 10].groupby(LABEL_COL)['amount_vs_avg'].hist(bins=30, alpha=0.7, ax=axes[0,0])\naxes[0,0].set_title('Amount vs Avg Ratio')\naxes[0,0].set_xlabel('Ratio')\naxes[0,0].legend(['Legit', 'Fraud'])\n\n# is_high_amount_vs_hist\ndf.groupby([LABEL_COL, 'is_high_amount_vs_hist']).size().unstack().plot(kind='bar', ax=axes[0,1])\naxes[0,1].set_title('High Amount vs History')\naxes[0,1].set_xlabel('Is Fraudulent')\n\n# high_velocity\ndf.groupby([LABEL_COL, 'high_velocity']).size().unstack().plot(kind='bar', ax=axes[0,2])\naxes[0,2].set_title('High Velocity')\naxes[0,2].set_xlabel('Is Fraudulent')\n\n# risky_payment\ndf.groupby([LABEL_COL, 'risky_payment']).size().unstack().plot(kind='bar', ax=axes[1,0])\naxes[1,0].set_title('Risky Payment Method')\naxes[1,0].set_xlabel('Is Fraudulent')\n\n# risky_category\ndf.groupby([LABEL_COL, 'risky_category']).size().unstack().plot(kind='bar', ax=axes[1,1])\naxes[1,1].set_title('Risky Category')\naxes[1,1].set_xlabel('Is Fraudulent')\n\n# address_mismatch\ndf.groupby([LABEL_COL, 'address_mismatch']).size().unstack().plot(kind='bar', ax=axes[1,2])\naxes[1,2].set_title('Address Mismatch')\naxes[1,2].set_xlabel('Is Fraudulent')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "all_features = primitives + calendar + address + behavioral + spending + risk\n",
    "print(\"\\nðŸ” Null Counts:\")\n",
    "null_counts = df[all_features].isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"âœ“ No nulls found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"\\nâˆž Infinite Values:\")\n",
    "numeric_features = [f for f in all_features if df[f].dtype in ['float64', 'int64']]\n",
    "inf_found = False\n",
    "for col in numeric_features:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"{col}: {inf_count}\")\n",
    "        inf_found = True\n",
    "if not inf_found:\n",
    "    print(\"âœ“ No infinite values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "print(\"\\nðŸ“ˆ Feature Statistics:\")\n",
    "df[all_features].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Correlations with Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate correlations\ncorrelations = df[numeric_features + [LABEL_COL]].corr()[LABEL_COL].drop(LABEL_COL).sort_values(ascending=False)\n\nprint(\"\\nðŸ”— Top 15 Features Correlated with Fraud:\")\nprint(correlations.head(15))\n\n# Plot\ncorrelations.head(20).plot(kind='barh', figsize=(10, 8), title='Top 20 Feature Correlations with Fraud')\nplt.xlabel('Correlation with Fraud')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample of complete feature matrix\nsample_cols = ['transaction_id', 'customer_id', 'amount'] + behavioral[:3] + spending[:4] + risk[:3] + [LABEL_COL]\nprint(\"\\nðŸ“‹ Sample Feature Matrix:\")\ndf[sample_cols].head(20)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Summary for Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE SUMMARY FOR FEAST\")\nprint(\"=\"*70)\n\nfeature_groups = {\n    'fraud_primitives': primitives,\n    'calendar_features': calendar,\n    'address_features': address,\n    'customer_behavioral_features': behavioral,\n    'customer_spending_features': spending,\n    'risk_indicators': risk,\n}\n\nfor group_name, features in feature_groups.items():\n    print(f\"\\n{group_name} ({len(features)} features):\")\n    for feat in features:\n        print(f\"  â€¢ {feat}\")\n\nprint(f\"\\nTotal Features: {len(all_features)}\")\nprint(f\"Label Column: {LABEL_COL} (COALESCE of reviewed_label and is_fraudulent)\")\nprint(\"\\nAll features are pre-computed in fraud_training_data table.\")\nprint(\"Feast reads this table for both training and serving.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\nâœ“ Exploration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**This notebook validated:**\n",
    "1. âœ“ All features computed by `mlops_training_features.py`\n",
    "2. âœ“ No nulls or infinite values\n",
    "3. âœ“ Feature distributions look reasonable\n",
    "4. âœ“ Strong correlations with fraud label\n",
    "\n",
    "**Feature categories:**\n",
    "- **Primitives** (10): Base transaction data\n",
    "- **Calendar** (4): Time-based flags\n",
    "- **Address** (4): Location risk indicators\n",
    "- **Behavioral** (7): Customer history patterns\n",
    "- **Spending** (8): Window aggregations (7d/30d/90d)\n",
    "- **Risk** (6): Composite risk signals\n",
    "\n",
    "**Next: Training**\n",
    "- Run `feast_training_job` in Dagster\n",
    "- Feast reads fraud_training_data table\n",
    "- Model trains on all features\n",
    "- Features materialized to Redis for serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}