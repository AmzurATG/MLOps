{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "MLOps/CVOps Platform - Complete Codebase Exploration\n",
    "\n",
    "This notebook provides hands-on exploration of every component in the platform.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Imports](#1-setup)\n",
    "2. [Settings & Configuration (Pydantic)](#2-settings)\n",
    "3. [Resources - LakeFS](#3-lakefs)\n",
    "4. [Resources - Trino/Iceberg](#4-trino)\n",
    "5. [Resources - Redis](#5-redis)\n",
    "6. [Resources - MLflow](#6-mlflow)\n",
    "7. [Feature Registry & Generator](#7-feature-registry)\n",
    "8. [Streaming - Kafka](#8-kafka)\n",
    "9. [Streaming - ksqlDB](#9-ksqldb)\n",
    "10. [API - Fraud Detection](#10-fraud-api)\n",
    "11. [API - Serving Layer](#11-serving)\n",
    "12. [Monitoring & Metrics](#12-monitoring)\n",
    "13. [Dagster Pipelines](#13-dagster)\n",
    "14. [End-to-End Flow Test](#14-e2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports <a id='1-setup'></a>\n",
    "\n",
    "First, let's set up the environment and import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Jupyter container\n",
    "# !pip install requests redis trino pandas pyyaml mlflow boto3 kafka-python feast --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/jovyan\n",
      "Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Environment configuration for Docker network\n# When running inside exp-jupyter container, use Docker service names\n# When running locally, use localhost with mapped ports\n\nIN_DOCKER = os.path.exists('/.dockerenv') or os.getenv('JUPYTER_ENABLE_LAB')\n\nif IN_DOCKER:\n    # Inside Docker network\n    LAKEFS_URL = \"http://exp-lakefs:8000\"\n    TRINO_HOST = \"exp-trino\"\n    TRINO_PORT = 8080\n    REDIS_HOST = \"exp-redis\"\n    REDIS_PORT = 6379\n    MLFLOW_URL = \"http://exp-mlflow:5000\"\n    KAFKA_BOOTSTRAP = \"exp-kafka:9092\"\n    KSQLDB_URL = \"http://exp-ksqldb-server:8088\"\n    FRAUD_API_URL = \"http://exp-fraud-api:8001\"\n    MINIO_ENDPOINT = \"http://exp-minio:9000\"\nelse:\n    # Local development with port mapping\n    LAKEFS_URL = \"http://localhost:18000\"\n    TRINO_HOST = \"localhost\"\n    TRINO_PORT = 18083\n    REDIS_HOST = \"localhost\"\n    REDIS_PORT = 16379\n    MLFLOW_URL = \"http://localhost:15000\"\n    KAFKA_BOOTSTRAP = \"localhost:29092\"\n    KSQLDB_URL = \"http://localhost:8088\"\n    FRAUD_API_URL = \"http://localhost:18002\"\n    MINIO_ENDPOINT = \"http://localhost:19000\"\n\n# LakeFS credentials - Get from environment or set in .env file\nLAKEFS_ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY_ID\", \"your-lakefs-access-key\")\nLAKEFS_SECRET_KEY = os.getenv(\"LAKEFS_SECRET_ACCESS_KEY\", \"your-lakefs-secret-key\")\n\nprint(f\"Running in Docker: {IN_DOCKER}\")\nprint(f\"LakeFS URL: {LAKEFS_URL}\")\nprint(f\"Trino: {TRINO_HOST}:{TRINO_PORT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Settings & Configuration (Pydantic) <a id='2-settings'></a>\n",
    "\n",
    "Explore the centralized settings system using Pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import settings: No module named 'pipelines'\n",
      "This is expected if pydantic-settings is not installed\n"
     ]
    }
   ],
   "source": [
    "# Import settings module\n",
    "try:\n",
    "    from pipelines.settings import settings, InfraSettings, MLOpsSettings, CVOpsSettings\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"INFRASTRUCTURE SETTINGS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Trino Host: {settings.infra.trino_host}\")\n",
    "    print(f\"Trino Port: {settings.infra.trino_port}\")\n",
    "    print(f\"Trino Catalog: {settings.infra.trino_catalog}\")\n",
    "    print(f\"Redis Host: {settings.infra.redis_host}\")\n",
    "    print(f\"MLflow URI: {settings.infra.mlflow_tracking_uri}\")\n",
    "    print(f\"Kafka Bootstrap: {settings.infra.kafka_bootstrap_servers}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MLOPS SETTINGS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MLflow Experiment: {settings.mlops.mlflow_experiment}\")\n",
    "    print(f\"Model Name: {settings.mlops.mlflow_model_name}\")\n",
    "    print(f\"Fraud API URL: {settings.mlops.fraud_api_url}\")\n",
    "    print(f\"AB Test Enabled: {settings.mlops.ab_test_enabled}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CVOPS SETTINGS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"CV Repo: {settings.cvops.cvops_repo}\")\n",
    "    print(f\"CV Branch: {settings.cvops.cvops_branch}\")\n",
    "    print(f\"YOLO Device: {settings.cvops.yolo_device}\")\n",
    "    print(f\"YOLO Confidence: {settings.cvops.yolo_confidence}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Could not import settings: {e}\")\n",
    "    print(\"This is expected if pydantic-settings is not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Pydantic model structure\n",
    "print(\"InfraSettings Fields:\")\n",
    "print(\"-\" * 40)\n",
    "for field_name, field_info in InfraSettings.model_fields.items():\n",
    "    default = field_info.default\n",
    "    print(f\"  {field_name}: {field_info.annotation} = {default}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Resources - LakeFS <a id='3-lakefs'></a>\n",
    "\n",
    "Explore the LakeFS resource for data versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LakeFS connection failed: name 'requests' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test LakeFS connection directly\n",
    "def test_lakefs_connection():\n",
    "    \"\"\"Test basic LakeFS API connectivity.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{LAKEFS_URL}/api/v1/healthcheck\",\n",
    "            timeout=5\n",
    "        )\n",
    "        print(f\"LakeFS Health: {response.status_code}\")\n",
    "        return response.status_code == 204\n",
    "    except Exception as e:\n",
    "        print(f\"LakeFS connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "lakefs_healthy = test_lakefs_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LakeFS API wrapper functions (matching resources.py)\n",
    "class LakeFSClient:\n",
    "    \"\"\"Simple LakeFS client for exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url, access_key, secret_key):\n",
    "        self.server_url = server_url\n",
    "        self.auth = (access_key, secret_key)\n",
    "    \n",
    "    def _api_call(self, method, endpoint, json_data=None):\n",
    "        url = f\"{self.server_url}/api/v1{endpoint}\"\n",
    "        response = requests.request(\n",
    "            method, url, auth=self.auth, json=json_data, timeout=30\n",
    "        )\n",
    "        if response.status_code == 404:\n",
    "            return None\n",
    "        response.raise_for_status()\n",
    "        return response.json() if response.content else {}\n",
    "    \n",
    "    def list_repositories(self):\n",
    "        \"\"\"List all LakeFS repositories.\"\"\"\n",
    "        return self._api_call(\"GET\", \"/repositories\")\n",
    "    \n",
    "    def get_repository(self, repo_name):\n",
    "        \"\"\"Get repository details.\"\"\"\n",
    "        return self._api_call(\"GET\", f\"/repositories/{repo_name}\")\n",
    "    \n",
    "    def list_branches(self, repo_name):\n",
    "        \"\"\"List branches in a repository.\"\"\"\n",
    "        return self._api_call(\"GET\", f\"/repositories/{repo_name}/branches\")\n",
    "    \n",
    "    def list_objects(self, repo_name, branch, prefix=\"\"):\n",
    "        \"\"\"List objects in a branch.\"\"\"\n",
    "        return self._api_call(\"GET\", f\"/repositories/{repo_name}/refs/{branch}/objects/ls?prefix={prefix}\")\n",
    "    \n",
    "    def get_commit_log(self, repo_name, branch, limit=5):\n",
    "        \"\"\"Get commit history.\"\"\"\n",
    "        return self._api_call(\"GET\", f\"/repositories/{repo_name}/refs/{branch}/commits?amount={limit}\")\n",
    "\n",
    "# Create client\n",
    "lakefs = LakeFSClient(LAKEFS_URL, LAKEFS_ACCESS_KEY, LAKEFS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all repositories\n",
    "if lakefs_healthy:\n",
    "    repos = lakefs.list_repositories()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LAKEFS REPOSITORIES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if repos and 'results' in repos:\n",
    "        for repo in repos['results']:\n",
    "            print(f\"\\nRepository: {repo['id']}\")\n",
    "            print(f\"  Storage: {repo.get('storage_namespace', 'N/A')}\")\n",
    "            print(f\"  Default Branch: {repo.get('default_branch', 'main')}\")\n",
    "            print(f\"  Created: {repo.get('creation_date', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"No repositories found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a specific repository (bronze or warehouse)\n",
    "REPO_TO_EXPLORE = \"bronze\"  # Change to explore different repos\n",
    "\n",
    "if lakefs_healthy:\n",
    "    repo_info = lakefs.get_repository(REPO_TO_EXPLORE)\n",
    "    if repo_info:\n",
    "        print(f\"Repository: {REPO_TO_EXPLORE}\")\n",
    "        pprint(repo_info)\n",
    "        \n",
    "        # List branches\n",
    "        branches = lakefs.list_branches(REPO_TO_EXPLORE)\n",
    "        print(f\"\\nBranches:\")\n",
    "        if branches and 'results' in branches:\n",
    "            for b in branches['results']:\n",
    "                print(f\"  - {b['id']} (commit: {b.get('commit_id', 'N/A')[:8]}...)\")\n",
    "    else:\n",
    "        print(f\"Repository '{REPO_TO_EXPLORE}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List objects in a branch\n",
    "BRANCH_TO_EXPLORE = \"main\"\n",
    "\n",
    "if lakefs_healthy:\n",
    "    objects = lakefs.list_objects(REPO_TO_EXPLORE, BRANCH_TO_EXPLORE)\n",
    "    if objects and 'results' in objects:\n",
    "        print(f\"Objects in {REPO_TO_EXPLORE}/{BRANCH_TO_EXPLORE}:\")\n",
    "        for obj in objects['results'][:20]:  # Limit to 20\n",
    "            path = obj.get('path', 'N/A')\n",
    "            size = obj.get('size_bytes', 0)\n",
    "            print(f\"  {path} ({size:,} bytes)\")\n",
    "        \n",
    "        if len(objects['results']) > 20:\n",
    "            print(f\"  ... and {len(objects['results']) - 20} more\")\n",
    "    else:\n",
    "        print(\"No objects found or branch empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View commit history\n",
    "if lakefs_healthy:\n",
    "    commits = lakefs.get_commit_log(REPO_TO_EXPLORE, BRANCH_TO_EXPLORE, limit=5)\n",
    "    if commits and 'results' in commits:\n",
    "        print(f\"Recent commits in {REPO_TO_EXPLORE}/{BRANCH_TO_EXPLORE}:\")\n",
    "        print(\"-\" * 60)\n",
    "        for commit in commits['results']:\n",
    "            commit_id = commit.get('id', 'N/A')[:8]\n",
    "            message = commit.get('message', 'No message')\n",
    "            committer = commit.get('committer', 'Unknown')\n",
    "            print(f\"{commit_id}... | {committer} | {message[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Resources - Trino/Iceberg <a id='4-trino'></a>\n",
    "\n",
    "Query Iceberg tables using Trino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication\n",
    "\n",
    "def get_trino_connection():\n",
    "    \"\"\"Create Trino connection.\"\"\"\n",
    "    return connect(\n",
    "        host=TRINO_HOST,\n",
    "        port=TRINO_PORT,\n",
    "        user=\"trino\",\n",
    "        catalog=\"iceberg_dev\",\n",
    "        schema=\"bronze\",\n",
    "    )\n",
    "\n",
    "def run_query(sql, fetch=True):\n",
    "    \"\"\"Execute Trino query and return results.\"\"\"\n",
    "    conn = get_trino_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    if fetch:\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return columns, rows\n",
    "    else:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return None, None\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    cols, rows = run_query(\"SELECT 1 as test\")\n",
    "    print(f\"Trino connection successful: {rows}\")\n",
    "    TRINO_HEALTHY = True\n",
    "except Exception as e:\n",
    "    print(f\"Trino connection failed: {e}\")\n",
    "    TRINO_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all schemas in iceberg_dev catalog\n",
    "if TRINO_HEALTHY:\n",
    "    cols, rows = run_query(\"SHOW SCHEMAS FROM iceberg_dev\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ICEBERG SCHEMAS\")\n",
    "    print(\"=\" * 60)\n",
    "    for row in rows:\n",
    "        print(f\"  {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in each schema\n",
    "if TRINO_HEALTHY:\n",
    "    schemas_to_check = ['bronze', 'silver', 'gold', 'evaluation', 'cv']\n",
    "    \n",
    "    for schema in schemas_to_check:\n",
    "        try:\n",
    "            cols, rows = run_query(f\"SHOW TABLES FROM iceberg_dev.{schema}\")\n",
    "            if rows:\n",
    "                print(f\"\\n{schema.upper()} TABLES:\")\n",
    "                for row in rows:\n",
    "                    print(f\"  - {row[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{schema}: Schema not found or empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data from fraud_transactions (if exists)\n",
    "if TRINO_HEALTHY:\n",
    "    try:\n",
    "        cols, rows = run_query(\"\"\"\n",
    "            SELECT * FROM iceberg_dev.bronze.fraud_transactions \n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"SAMPLE: bronze.fraud_transactions\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Columns: {cols}\")\n",
    "        print(f\"\\nRows:\")\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Table not found or empty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table row counts\n",
    "if TRINO_HEALTHY:\n",
    "    tables_to_count = [\n",
    "        ('bronze', 'fraud_transactions'),\n",
    "        ('silver', 'fraud_enriched'),\n",
    "        ('gold', 'fraud_training_data'),\n",
    "    ]\n",
    "    \n",
    "    print(\"TABLE ROW COUNTS\")\n",
    "    print(\"-\" * 40)\n",
    "    for schema, table in tables_to_count:\n",
    "        try:\n",
    "            cols, rows = run_query(f\"SELECT COUNT(*) FROM iceberg_dev.{schema}.{table}\")\n",
    "            count = rows[0][0]\n",
    "            print(f\"{schema}.{table}: {count:,} rows\")\n",
    "        except:\n",
    "            print(f\"{schema}.{table}: (not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Iceberg table history (snapshots)\n",
    "if TRINO_HEALTHY:\n",
    "    try:\n",
    "        cols, rows = run_query(\"\"\"\n",
    "            SELECT snapshot_id, committed_at, operation, summary\n",
    "            FROM iceberg_dev.bronze.\"fraud_transactions$snapshots\"\n",
    "            ORDER BY committed_at DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"ICEBERG SNAPSHOTS (fraud_transactions)\")\n",
    "        print(\"-\" * 60)\n",
    "        for row in rows:\n",
    "            print(f\"Snapshot: {row[0]}\")\n",
    "            print(f\"  Time: {row[1]}\")\n",
    "            print(f\"  Operation: {row[2]}\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not query snapshots: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Resources - Redis <a id='5-redis'></a>\n",
    "\n",
    "Explore Redis for streaming features and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "try:\n",
    "    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0, decode_responses=True)\n",
    "    r.ping()\n",
    "    print(f\"Redis connection successful!\")\n",
    "    print(f\"Redis info: {r.info('server')['redis_version']}\")\n",
    "    REDIS_HEALTHY = True\n",
    "except Exception as e:\n",
    "    print(f\"Redis connection failed: {e}\")\n",
    "    REDIS_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Redis keys\n",
    "if REDIS_HEALTHY:\n",
    "    # Get all keys (limited)\n",
    "    all_keys = r.keys('*')\n",
    "    print(f\"Total keys in Redis: {len(all_keys)}\")\n",
    "    \n",
    "    # Group by prefix\n",
    "    prefixes = {}\n",
    "    for key in all_keys:\n",
    "        prefix = key.split(':')[0] if ':' in key else key\n",
    "        prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
    "    \n",
    "    print(\"\\nKeys by prefix:\")\n",
    "    for prefix, count in sorted(prefixes.items()):\n",
    "        print(f\"  {prefix}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check streaming features (from Kafka consumer)\n",
    "if REDIS_HEALTHY:\n",
    "    streaming_keys = r.keys('feast:streaming:*')\n",
    "    print(f\"Streaming feature keys: {len(streaming_keys)}\")\n",
    "    \n",
    "    if streaming_keys:\n",
    "        # Sample one key\n",
    "        sample_key = streaming_keys[0]\n",
    "        print(f\"\\nSample key: {sample_key}\")\n",
    "        \n",
    "        key_type = r.type(sample_key)\n",
    "        print(f\"Type: {key_type}\")\n",
    "        \n",
    "        if key_type == 'hash':\n",
    "            data = r.hgetall(sample_key)\n",
    "            print(f\"Data:\")\n",
    "            for k, v in data.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        elif key_type == 'string':\n",
    "            data = r.get(sample_key)\n",
    "            print(f\"Value: {data}\")\n",
    "        \n",
    "        ttl = r.ttl(sample_key)\n",
    "        print(f\"TTL: {ttl} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read test data\n",
    "if REDIS_HEALTHY:\n",
    "    # Write a test hash\n",
    "    test_key = \"notebook:test:customer_123\"\n",
    "    test_data = {\n",
    "        \"tx_count_5min\": \"3\",\n",
    "        \"amount_sum_5min\": \"450.00\",\n",
    "        \"velocity_score\": \"0.6\",\n",
    "        \"high_velocity_flag\": \"0\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    r.hset(test_key, mapping=test_data)\n",
    "    r.expire(test_key, 300)  # 5 minute TTL\n",
    "    \n",
    "    # Read it back\n",
    "    retrieved = r.hgetall(test_key)\n",
    "    print(\"Wrote and retrieved test data:\")\n",
    "    pprint(retrieved)\n",
    "    \n",
    "    # Clean up\n",
    "    r.delete(test_key)\n",
    "    print(f\"\\nCleaned up test key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Resources - MLflow <a id='6-mlflow'></a>\n",
    "\n",
    "Explore MLflow for model registry and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_URL)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    experiments = client.search_experiments()\n",
    "    print(f\"MLflow connection successful!\")\n",
    "    print(f\"Found {len(experiments)} experiments\")\n",
    "    MLFLOW_HEALTHY = True\n",
    "except Exception as e:\n",
    "    print(f\"MLflow connection failed: {e}\")\n",
    "    MLFLOW_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "if MLFLOW_HEALTHY:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLFLOW EXPERIMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\nExperiment: {exp.name}\")\n",
    "        print(f\"  ID: {exp.experiment_id}\")\n",
    "        print(f\"  Artifact Location: {exp.artifact_location}\")\n",
    "        print(f\"  Lifecycle Stage: {exp.lifecycle_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List registered models\n",
    "if MLFLOW_HEALTHY:\n",
    "    models = client.search_registered_models()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"REGISTERED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if models:\n",
    "        for model in models:\n",
    "            print(f\"\\nModel: {model.name}\")\n",
    "            print(f\"  Description: {model.description or 'N/A'}\")\n",
    "            \n",
    "            # Get latest versions\n",
    "            for version in model.latest_versions:\n",
    "                print(f\"  Version {version.version}:\")\n",
    "                print(f\"    Stage: {version.current_stage}\")\n",
    "                print(f\"    Run ID: {version.run_id[:8]}...\")\n",
    "    else:\n",
    "        print(\"No registered models found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runs from fraud-detection experiment\n",
    "if MLFLOW_HEALTHY:\n",
    "    try:\n",
    "        exp = client.get_experiment_by_name(\"fraud-detection\")\n",
    "        if exp:\n",
    "            runs = client.search_runs(\n",
    "                experiment_ids=[exp.experiment_id],\n",
    "                max_results=5,\n",
    "                order_by=[\"start_time DESC\"]\n",
    "            )\n",
    "            \n",
    "            print(\"Recent runs from 'fraud-detection' experiment:\")\n",
    "            print(\"-\" * 60)\n",
    "            for run in runs:\n",
    "                print(f\"\\nRun: {run.info.run_id[:8]}...\")\n",
    "                print(f\"  Status: {run.info.status}\")\n",
    "                print(f\"  Start: {run.info.start_time}\")\n",
    "                \n",
    "                # Show key metrics\n",
    "                metrics = run.data.metrics\n",
    "                if 'accuracy' in metrics:\n",
    "                    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "                if 'f1_score' in metrics:\n",
    "                    print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        else:\n",
    "            print(\"Experiment 'fraud-detection' not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model (if available)\n",
    "if MLFLOW_HEALTHY:\n",
    "    try:\n",
    "        # Try to load production model\n",
    "        model_uri = \"models:/fraud-detector/Production\"\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "        print(f\"Loaded model from: {model_uri}\")\n",
    "        print(f\"Model type: {type(model)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load production model: {e}\")\n",
    "        print(\"This is expected if no model is in Production stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Registry & Generator <a id='7-feature-registry'></a>\n",
    "\n",
    "Explore the feature registry YAML and code generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load feature registry\n",
    "registry_path = os.path.join(PROJECT_ROOT, 'feature_registry', 'fraud_detection.yaml')\n",
    "\n",
    "with open(registry_path, 'r') as f:\n",
    "    registry = yaml.safe_load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE REGISTRY STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project: {registry['project']['name']}\")\n",
    "print(f\"Version: {registry['project']['version']}\")\n",
    "print(f\"Description: {registry['project']['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore entities\n",
    "print(\"ENTITIES\")\n",
    "print(\"-\" * 40)\n",
    "for name, entity in registry['entities'].items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Join Key: {entity['join_key']}\")\n",
    "    print(f\"    Type: {entity['type']}\")\n",
    "    print(f\"    Description: {entity['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature groups\n",
    "print(\"FEATURE GROUPS\")\n",
    "print(\"-\" * 40)\n",
    "for group_name, group in registry['features'].items():\n",
    "    columns = group.get('columns', [])\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    print(f\"  Entity: {group.get('entity', 'N/A')}\")\n",
    "    print(f\"  TTL Days: {group.get('ttl_days', 'N/A')}\")\n",
    "    print(f\"  Online: {group.get('online', False)}\")\n",
    "    print(f\"  Columns: {len(columns)}\")\n",
    "    \n",
    "    # Show first 3 columns as sample\n",
    "    for col in columns[:3]:\n",
    "        print(f\"    - {col['name']}: {col['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore streaming configuration\n",
    "if 'streaming' in registry:\n",
    "    streaming = registry['streaming']\n",
    "    print(\"STREAMING CONFIGURATION\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Source Topic: {streaming.get('source_topic')}\")\n",
    "    print(f\"Output Topic: {streaming.get('output_topic')}\")\n",
    "    print(f\"Value Format: {streaming.get('value_format')}\")\n",
    "    \n",
    "    # Windows\n",
    "    print(f\"\\nWindows:\")\n",
    "    for window in streaming.get('windows', []):\n",
    "        duration = window['duration_seconds']\n",
    "        agg_count = len(window.get('aggregations', []))\n",
    "        print(f\"  {window['name']}: {duration}s, {agg_count} aggregations\")\n",
    "    \n",
    "    # Computed flags\n",
    "    print(f\"\\nComputed Flags:\")\n",
    "    for flag in streaming.get('computed_flags', []):\n",
    "        print(f\"  {flag['name']}: {flag['logic']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generator\n",
    "try:\n",
    "    from feature_registry.generator import FeatureGenerator\n",
    "    \n",
    "    generator = FeatureGenerator.from_yaml(registry_path)\n",
    "    \n",
    "    print(\"Feature Generator loaded successfully!\")\n",
    "    print(f\"Project: {generator.registry.project_name}\")\n",
    "    print(f\"Source Table: {generator.registry.source_table}\")\n",
    "    print(f\"Output Table: {generator.registry.output_table}\")\n",
    "    print(f\"Feature Groups: {len(generator.registry.feature_groups)}\")\n",
    "    print(f\"Feature Order: {len(generator.registry.feature_order)} features\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import generator: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SQL sample\n",
    "try:\n",
    "    sql = generator.generate_sql()\n",
    "    print(\"GENERATED SQL (first 100 lines):\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, line in enumerate(sql.split('\\n')[:100]):\n",
    "        print(f\"{i+1:3d} | {line}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ksqlDB sample\n",
    "try:\n",
    "    if generator.registry.streaming:\n",
    "        ksql = generator.generate_ksqldb_streams()\n",
    "        print(\"GENERATED ksqlDB STREAMS (first 50 lines):\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, line in enumerate(ksql.split('\\n')[:50]):\n",
    "            print(f\"{i+1:3d} | {line}\")\n",
    "    else:\n",
    "        print(\"No streaming config in registry\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Streaming - Kafka <a id='8-kafka'></a>\n",
    "\n",
    "Explore Kafka topics and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAdminClient, KafkaConsumer, KafkaProducer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NewTopic\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Connect to Kafka\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaConsumer, KafkaProducer\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "# Connect to Kafka\n",
    "try:\n",
    "    admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP)\n",
    "    print(f\"Kafka connection successful!\")\n",
    "    KAFKA_HEALTHY = True\n",
    "except Exception as e:\n",
    "    print(f\"Kafka connection failed: {e}\")\n",
    "    KAFKA_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all topics\n",
    "if KAFKA_HEALTHY:\n",
    "    topics = admin.list_topics()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KAFKA TOPICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group by prefix\n",
    "    fraud_topics = [t for t in topics if t.startswith('fraud')]\n",
    "    cv_topics = [t for t in topics if t.startswith('cv')]\n",
    "    debezium_topics = [t for t in topics if t.startswith('debezium')]\n",
    "    internal_topics = [t for t in topics if t.startswith('_')]\n",
    "    other_topics = [t for t in topics if t not in fraud_topics + cv_topics + debezium_topics + internal_topics]\n",
    "    \n",
    "    print(f\"\\nFraud Topics ({len(fraud_topics)}):\")\n",
    "    for t in fraud_topics:\n",
    "        print(f\"  - {t}\")\n",
    "    \n",
    "    print(f\"\\nCV Topics ({len(cv_topics)}):\")\n",
    "    for t in cv_topics:\n",
    "        print(f\"  - {t}\")\n",
    "    \n",
    "    print(f\"\\nDebezium Topics ({len(debezium_topics)}):\")\n",
    "    for t in debezium_topics:\n",
    "        print(f\"  - {t}\")\n",
    "    \n",
    "    print(f\"\\nOther Topics ({len(other_topics)}):\")\n",
    "    for t in other_topics:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consume sample messages from a topic\n",
    "def sample_topic(topic_name, max_messages=3, timeout_ms=5000):\n",
    "    \"\"\"Consume sample messages from a topic.\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        topic_name,\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP,\n",
    "        auto_offset_reset='earliest',\n",
    "        consumer_timeout_ms=timeout_ms,\n",
    "        value_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
    "    )\n",
    "    \n",
    "    messages = []\n",
    "    for msg in consumer:\n",
    "        messages.append({\n",
    "            'partition': msg.partition,\n",
    "            'offset': msg.offset,\n",
    "            'key': msg.key.decode('utf-8') if msg.key else None,\n",
    "            'value': msg.value[:500] if msg.value and len(msg.value) > 500 else msg.value,\n",
    "        })\n",
    "        if len(messages) >= max_messages:\n",
    "            break\n",
    "    \n",
    "    consumer.close()\n",
    "    return messages\n",
    "\n",
    "# Sample fraud CDC topic\n",
    "if KAFKA_HEALTHY and 'fraud.demo.fraud_transactions' in topics:\n",
    "    print(\"Sample messages from fraud.demo.fraud_transactions:\")\n",
    "    print(\"-\" * 60)\n",
    "    msgs = sample_topic('fraud.demo.fraud_transactions', max_messages=2)\n",
    "    for msg in msgs:\n",
    "        print(f\"\\nPartition: {msg['partition']}, Offset: {msg['offset']}\")\n",
    "        try:\n",
    "            value = json.loads(msg['value'])\n",
    "            pprint(value)\n",
    "        except:\n",
    "            print(msg['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a test message\n",
    "if KAFKA_HEALTHY:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP,\n",
    "        value_serializer=lambda x: json.dumps(x).encode('utf-8'),\n",
    "    )\n",
    "    \n",
    "    test_topic = 'notebook.test.topic'\n",
    "    test_message = {\n",
    "        'source': 'exploration_notebook',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'data': {'test_field': 'test_value'}\n",
    "    }\n",
    "    \n",
    "    future = producer.send(test_topic, value=test_message)\n",
    "    result = future.get(timeout=10)\n",
    "    \n",
    "    print(f\"Produced message to {test_topic}\")\n",
    "    print(f\"  Partition: {result.partition}\")\n",
    "    print(f\"  Offset: {result.offset}\")\n",
    "    \n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Streaming - ksqlDB <a id='9-ksqldb'></a>\n",
    "\n",
    "Explore ksqlDB streams and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksql_query(statement):\n",
    "    \"\"\"Execute ksqlDB statement.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{KSQLDB_URL}/ksql\",\n",
    "        json={\"ksql\": statement, \"streamsProperties\": {}},\n",
    "        headers={\"Content-Type\": \"application/vnd.ksql.v1+json\"},\n",
    "        timeout=30,\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = requests.get(f\"{KSQLDB_URL}/healthcheck\", timeout=5)\n",
    "    print(f\"ksqlDB health: {response.status_code}\")\n",
    "    KSQLDB_HEALTHY = response.status_code == 200\n",
    "except Exception as e:\n",
    "    print(f\"ksqlDB connection failed: {e}\")\n",
    "    KSQLDB_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List streams\n",
    "if KSQLDB_HEALTHY:\n",
    "    result = ksql_query(\"SHOW STREAMS;\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KSQLDB STREAMS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if result and len(result) > 0:\n",
    "        streams = result[0].get('streams', [])\n",
    "        for stream in streams:\n",
    "            print(f\"\\n{stream.get('name', 'N/A')}\")\n",
    "            print(f\"  Topic: {stream.get('topic', 'N/A')}\")\n",
    "            print(f\"  Format: {stream.get('keyFormat', 'N/A')} / {stream.get('valueFormat', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables (aggregations)\n",
    "if KSQLDB_HEALTHY:\n",
    "    result = ksql_query(\"SHOW TABLES;\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KSQLDB TABLES (Aggregations)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if result and len(result) > 0:\n",
    "        tables = result[0].get('tables', [])\n",
    "        for table in tables:\n",
    "            print(f\"\\n{table.get('name', 'N/A')}\")\n",
    "            print(f\"  Topic: {table.get('topic', 'N/A')}\")\n",
    "            print(f\"  Windowed: {table.get('isWindowed', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a stream schema\n",
    "if KSQLDB_HEALTHY:\n",
    "    stream_name = \"TRANSACTIONS_ENRICHED\"  # Change as needed\n",
    "    \n",
    "    try:\n",
    "        result = ksql_query(f\"DESCRIBE {stream_name};\")\n",
    "        \n",
    "        if result and len(result) > 0:\n",
    "            info = result[0].get('sourceDescription', {})\n",
    "            fields = info.get('fields', [])\n",
    "            \n",
    "            print(f\"Schema for {stream_name}:\")\n",
    "            print(\"-\" * 40)\n",
    "            for field in fields:\n",
    "                print(f\"  {field['name']}: {field['schema']['type']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Stream not found or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. API - Fraud Detection <a id='10-fraud-api'></a>\n",
    "\n",
    "Test the Fraud Detection API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API health\n",
    "try:\n",
    "    response = requests.get(f\"{FRAUD_API_URL}/health\", timeout=5)\n",
    "    print(f\"Fraud API Health: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        pprint(response.json())\n",
    "    FRAUD_API_HEALTHY = response.status_code == 200\n",
    "except Exception as e:\n",
    "    print(f\"Fraud API connection failed: {e}\")\n",
    "    FRAUD_API_HEALTHY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API documentation\n",
    "if FRAUD_API_HEALTHY:\n",
    "    response = requests.get(f\"{FRAUD_API_URL}/openapi.json\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        openapi = response.json()\n",
    "        print(\"API Endpoints:\")\n",
    "        print(\"-\" * 40)\n",
    "        for path, methods in openapi.get('paths', {}).items():\n",
    "            for method in methods.keys():\n",
    "                if method in ['get', 'post', 'put', 'delete']:\n",
    "                    print(f\"  {method.upper():6s} {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction request\n",
    "if FRAUD_API_HEALTHY:\n",
    "    prediction_request = {\n",
    "        \"transaction_id\": \"notebook_test_001\",\n",
    "        \"customer_id\": \"cust_notebook_test\",\n",
    "        \"amount\": 150.00,\n",
    "        \"quantity\": 2,\n",
    "        \"country\": \"US\",\n",
    "        \"device_type\": \"mobile\",\n",
    "        \"payment_method\": \"credit_card\",\n",
    "        \"category\": \"Electronics\",\n",
    "        \"account_age_days\": 365,\n",
    "        \"tx_hour\": 14,\n",
    "        \"tx_dayofweek\": 2\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{FRAUD_API_URL}/predict\",\n",
    "            json=prediction_request,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        print(\"Prediction Request:\")\n",
    "        pprint(prediction_request)\n",
    "        print(\"\\nPrediction Response:\")\n",
    "        pprint(response.json())\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction\n",
    "if FRAUD_API_HEALTHY:\n",
    "    batch_request = {\n",
    "        \"transactions\": [\n",
    "            {\n",
    "                \"transaction_id\": f\"batch_{i}\",\n",
    "                \"customer_id\": f\"cust_{i}\",\n",
    "                \"amount\": 100.0 + i * 50,\n",
    "                \"quantity\": 1,\n",
    "                \"country\": \"US\",\n",
    "                \"device_type\": \"desktop\",\n",
    "                \"payment_method\": \"credit_card\",\n",
    "                \"category\": \"Electronics\",\n",
    "                \"account_age_days\": 100,\n",
    "                \"tx_hour\": 10,\n",
    "                \"tx_dayofweek\": 3\n",
    "            }\n",
    "            for i in range(5)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{FRAUD_API_URL}/predict/batch\",\n",
    "            json=batch_request,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        print(\"Batch Prediction Results:\")\n",
    "        results = response.json()\n",
    "        for r in results.get('predictions', results)[:5]:\n",
    "            print(f\"  {r.get('transaction_id')}: score={r.get('fraud_score', r.get('score', 'N/A'))}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Batch prediction failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. API - Serving Layer <a id='11-serving'></a>\n",
    "\n",
    "Explore the modular serving layer components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import serving layer modules\n",
    "try:\n",
    "    from api.serving.features.feature_service import FeatureService\n",
    "    from api.serving.models.model_loader import ModelLoader\n",
    "    from api.serving.scoring.adjustments import ScoreAdjuster\n",
    "    \n",
    "    print(\"Serving layer modules imported successfully!\")\n",
    "    print(\"\\nAvailable classes:\")\n",
    "    print(\"  - FeatureService: Unified feature retrieval (Feast + Redis)\")\n",
    "    print(\"  - ModelLoader: MLflow model loading with caching\")\n",
    "    print(\"  - ScoreAdjuster: Score adjustments and business rules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import serving modules: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore FeatureService\n",
    "try:\n",
    "    import inspect\n",
    "    from api.serving.features.feature_service import FeatureService\n",
    "    \n",
    "    print(\"FeatureService Methods:\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, method in inspect.getmembers(FeatureService, predicate=inspect.isfunction):\n",
    "        if not name.startswith('_'):\n",
    "            sig = inspect.signature(method)\n",
    "            print(f\"  {name}{sig}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read serving layer source code\n",
    "feature_service_path = os.path.join(PROJECT_ROOT, 'api', 'serving', 'features', 'feature_service.py')\n",
    "\n",
    "if os.path.exists(feature_service_path):\n",
    "    with open(feature_service_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(\"FeatureService Source (first 80 lines):\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, line in enumerate(content.split('\\n')[:80]):\n",
    "        print(f\"{i+1:3d} | {line}\")\n",
    "else:\n",
    "    print(f\"File not found: {feature_service_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Monitoring & Metrics <a id='12-monitoring'></a>\n",
    "\n",
    "Explore the monitoring infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import monitoring utilities\n",
    "try:\n",
    "    from src.core.monitoring import get_or_create_counter, get_or_create_histogram, track_execution\n",
    "    \n",
    "    print(\"Monitoring utilities loaded!\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"  - get_or_create_counter: Safe Prometheus counter creation\")\n",
    "    print(\"  - get_or_create_histogram: Safe Prometheus histogram creation\")\n",
    "    print(\"  - track_execution: Decorator for automatic metrics tracking\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import monitoring: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Prometheus metrics endpoint (if API exposes it)\n",
    "if FRAUD_API_HEALTHY:\n",
    "    try:\n",
    "        response = requests.get(f\"{FRAUD_API_URL}/metrics\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Prometheus Metrics (sample):\")\n",
    "            print(\"-\" * 60)\n",
    "            # Show first 30 lines\n",
    "            for line in response.text.split('\\n')[:30]:\n",
    "                print(line)\n",
    "        else:\n",
    "            print(f\"Metrics endpoint returned: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Metrics endpoint not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read monitoring configuration\n",
    "prometheus_config = os.path.join(PROJECT_ROOT, 'monitoring', 'prometheus', 'prometheus.yml')\n",
    "\n",
    "if os.path.exists(prometheus_config):\n",
    "    with open(prometheus_config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Prometheus Scrape Configs:\")\n",
    "    print(\"-\" * 40)\n",
    "    for job in config.get('scrape_configs', []):\n",
    "        print(f\"\\nJob: {job.get('job_name')}\")\n",
    "        targets = job.get('static_configs', [{}])[0].get('targets', [])\n",
    "        for target in targets:\n",
    "            print(f\"  - {target}\")\n",
    "else:\n",
    "    print(\"Prometheus config not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Dagster Pipelines <a id='13-dagster'></a>\n",
    "\n",
    "Explore Dagster assets and jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List pipeline files\n",
    "pipelines_dir = os.path.join(PROJECT_ROOT, 'pipelines')\n",
    "\n",
    "print(\"Pipeline Files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for f in sorted(os.listdir(pipelines_dir)):\n",
    "    if f.endswith('.py') and not f.startswith('__'):\n",
    "        filepath = os.path.join(pipelines_dir, f)\n",
    "        size = os.path.getsize(filepath)\n",
    "        \n",
    "        # Count lines\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = len(file.readlines())\n",
    "        \n",
    "        print(f\"  {f:40s} {lines:5d} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore MLOps pipeline structure\n",
    "mlops_path = os.path.join(PROJECT_ROOT, 'pipelines', 'mlops.py')\n",
    "\n",
    "if os.path.exists(mlops_path):\n",
    "    with open(mlops_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Find all @asset decorated functions\n",
    "    import re\n",
    "    assets = re.findall(r'@asset[^\\n]*\\ndef (\\w+)', content)\n",
    "    \n",
    "    print(\"Assets in mlops.py:\")\n",
    "    print(\"-\" * 40)\n",
    "    for asset in assets:\n",
    "        print(f\"  - {asset}\")\n",
    "    \n",
    "    # Find all sensors\n",
    "    sensors = re.findall(r'@sensor[^\\n]*\\ndef (\\w+)', content)\n",
    "    if sensors:\n",
    "        print(\"\\nSensors:\")\n",
    "        for sensor in sensors:\n",
    "            print(f\"  - {sensor}\")\n",
    "    \n",
    "    # Find all jobs\n",
    "    jobs = re.findall(r'define_asset_job\\([\"\\']([\\w_]+)', content)\n",
    "    if jobs:\n",
    "        print(\"\\nJobs:\")\n",
    "        for job in jobs:\n",
    "            print(f\"  - {job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Dagster definitions file\n",
    "definitions_path = os.path.join(PROJECT_ROOT, 'pipelines', 'definitions.py')\n",
    "\n",
    "if os.path.exists(definitions_path):\n",
    "    with open(definitions_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(\"Dagster Definitions (first 100 lines):\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, line in enumerate(content.split('\\n')[:100]):\n",
    "        print(f\"{i+1:3d} | {line}\")\n",
    "else:\n",
    "    print(\"definitions.py not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. End-to-End Flow Test <a id='14-e2e'></a>\n",
    "\n",
    "Test the complete data flow from ingestion to inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all connections\n",
    "print(\"=\" * 60)\n",
    "print(\"PLATFORM HEALTH SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "services = [\n",
    "    (\"LakeFS\", LAKEFS_URL, lakefs_healthy if 'lakefs_healthy' in dir() else False),\n",
    "    (\"Trino\", f\"{TRINO_HOST}:{TRINO_PORT}\", TRINO_HEALTHY if 'TRINO_HEALTHY' in dir() else False),\n",
    "    (\"Redis\", f\"{REDIS_HOST}:{REDIS_PORT}\", REDIS_HEALTHY if 'REDIS_HEALTHY' in dir() else False),\n",
    "    (\"MLflow\", MLFLOW_URL, MLFLOW_HEALTHY if 'MLFLOW_HEALTHY' in dir() else False),\n",
    "    (\"Kafka\", KAFKA_BOOTSTRAP, KAFKA_HEALTHY if 'KAFKA_HEALTHY' in dir() else False),\n",
    "    (\"ksqlDB\", KSQLDB_URL, KSQLDB_HEALTHY if 'KSQLDB_HEALTHY' in dir() else False),\n",
    "    (\"Fraud API\", FRAUD_API_URL, FRAUD_API_HEALTHY if 'FRAUD_API_HEALTHY' in dir() else False),\n",
    "]\n",
    "\n",
    "for name, url, healthy in services:\n",
    "    status = \"\" if healthy else \"\"\n",
    "    print(f\"{status} {name:12s} | {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data flow diagram\n",
    "print(\"\"\"\n",
    "=============================================================================\n",
    "                        END-TO-END DATA FLOW\n",
    "=============================================================================\n",
    "\n",
    "1. DATA INGESTION\n",
    "   MySQL  Debezium CDC  Kafka (fraud.demo.fraud_transactions)\n",
    "\n",
    "2. BATCH PROCESSING  \n",
    "   Kafka  Dagster (mlops_bronze)  Trino/Iceberg (bronze.fraud_transactions)\n",
    "          Dagster (mlops_gold)  Trino/Iceberg (gold.fraud_training_data)\n",
    "\n",
    "3. STREAMING PROCESSING\n",
    "   Kafka  ksqlDB (enrichment + windowing)  Kafka (fraud.streaming.features)\n",
    "          Dagster Sensor  Redis (feast:streaming:*)\n",
    "\n",
    "4. FEATURE STORE\n",
    "   Trino/Iceberg  Feast (offline store)\n",
    "   Redis  Feast (online store - streaming features)\n",
    "\n",
    "5. MODEL TRAINING\n",
    "   Feast (offline)  Training  MLflow (model registry)\n",
    "\n",
    "6. INFERENCE\n",
    "   API Request  Feature Service (Feast + Redis)  Model  Prediction\n",
    "\n",
    "7. MONITORING\n",
    "   All services  Prometheus  Grafana\n",
    "\n",
    "=============================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=============================================================================\n",
    "                     EXPLORATION COMPLETE!\n",
    "=============================================================================\n",
    "\n",
    "You've explored:\n",
    "\n",
    " Settings & Configuration (Pydantic)\n",
    " LakeFS (Data Versioning)\n",
    " Trino/Iceberg (Data Warehouse)\n",
    " Redis (Caching & Streaming Features)\n",
    " MLflow (Model Registry)\n",
    " Feature Registry & Generator\n",
    " Kafka (Event Streaming)\n",
    " ksqlDB (Stream Processing)\n",
    " Fraud Detection API\n",
    " Serving Layer Architecture\n",
    " Monitoring Infrastructure\n",
    " Dagster Pipelines\n",
    "\n",
    "Next Steps:\n",
    "1. Modify cells to explore specific areas in more depth\n",
    "2. Try the Quick Test commands in STREAMING_ARCHITECTURE.md\n",
    "3. Run the full pipeline using Dagster UI (http://localhost:13000)\n",
    "4. Check Grafana dashboards (http://localhost:3002)\n",
    "\n",
    "=============================================================================\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}