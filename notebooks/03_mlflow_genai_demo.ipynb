{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MLflow GenAI Demo: LLM Observability & Experiment Tracking\n\nThis notebook demonstrates MLflow's GenAI capabilities for tracking, tracing, and evaluating LLM applications.\n\n## Features Demonstrated\n\n### Part 1: Via LiteLLM Proxy\n1. **Auto-logging** - LiteLLM → MLflow automatic trace logging\n2. **@mlflow.trace** - Decorator for function-level tracing\n3. **start_span()** - Custom spans for multi-step pipelines\n\n### Part 2: MLflow Standalone (Direct SDK)\n4. **OpenAI Autolog** - `mlflow.openai.autolog()`\n5. **Direct Groq SDK** - Native tracing without proxy\n6. **Manual Logging** - Full control over what gets tracked\n\n### Part 3: Advanced GenAI Features\n7. **Prompt Registry** - Version control for prompts (MLflow UI → Prompts tab)\n8. **Model Evaluation** - `mlflow.genai.evaluate()` with scorers\n9. **A/B Testing** - Compare models and prompts with metrics\n\n## Prerequisites\n```bash\ncd deploy/docker/compose\ndocker compose -f docker-compose.core.yml up -d exp-mlflow exp-postgres-mlflow exp-minio\ndocker compose -f docker-compose.litellm.yml up -d  # Optional for Part 1\n```\n\n## MLflow UI Tabs\n| Tab | What It Shows |\n|-----|---------------|\n| **Experiments** | All runs with params/metrics |\n| **Traces** | LLM call hierarchies and spans |\n| **Prompts** | Registered prompts with version history |\n| **Evaluation** | Model evaluation results |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "**IMPORTANT**: S3/MinIO credentials must be set BEFORE importing mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CELL 1: Set credentials BEFORE importing mlflow\n# ============================================================================\nimport os\nimport socket\n\ndef detect_environment():\n    \"\"\"Detect if running inside Docker or locally.\"\"\"\n    try:\n        socket.create_connection((\"exp-mlflow\", 5000), timeout=1)\n        return \"docker\"\n    except (socket.error, socket.timeout):\n        return \"local\"\n\nENV = detect_environment()\n\n# Service URLs based on environment\nif ENV == \"docker\":\n    MLFLOW_URI = \"http://exp-mlflow:5000\"\n    LITELLM_URL = \"http://exp-litellm:4000\"\n    MINIO_URL = \"http://exp-minio:9000\"\nelse:\n    MLFLOW_URI = \"http://localhost:15000\"\n    LITELLM_URL = \"http://localhost:4000\"\n    MINIO_URL = \"http://localhost:19000\"\n\n# Set S3/MinIO credentials (MUST be before mlflow import)\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"admin\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"password123\"\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MINIO_URL\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\n# API Keys for standalone usage - Get from environment or set your own\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY_HERE\")\nLITELLM_API_KEY = \"sk-local-dev-2025\"\n\nprint(f\"Environment: {ENV}\")\nprint(f\"MLflow: {MLFLOW_URI}\")\nprint(f\"LiteLLM: {LITELLM_URL}\")\nprint(f\"MinIO: {MINIO_URL}\")\nprint(f\"Groq API Key: {'✓ Set' if GROQ_API_KEY and GROQ_API_KEY != 'YOUR_GROQ_API_KEY_HERE' else '✗ Missing - set GROQ_API_KEY env var'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import libraries AFTER setting credentials\n",
    "# ============================================================================\n",
    "import mlflow\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Health Checks\n",
    "# ============================================================================\n",
    "def check_services():\n",
    "    results = {}\n",
    "    \n",
    "    # Check MLflow\n",
    "    try:\n",
    "        resp = requests.get(f\"{MLFLOW_URI}/health\", timeout=5)\n",
    "        results[\"MLflow\"] = \"OK\" if resp.status_code == 200 else f\"Error: {resp.status_code}\"\n",
    "    except Exception as e:\n",
    "        results[\"MLflow\"] = f\"FAILED: {str(e)[:40]}\"\n",
    "    \n",
    "    # Check LiteLLM (optional)\n",
    "    try:\n",
    "        resp = requests.get(f\"{LITELLM_URL}/health/liveliness\", timeout=5)\n",
    "        results[\"LiteLLM\"] = \"OK\" if resp.status_code == 200 else f\"Error: {resp.status_code}\"\n",
    "    except Exception as e:\n",
    "        results[\"LiteLLM\"] = f\"Not running (OK for Part 2)\"\n",
    "    \n",
    "    # Check MinIO\n",
    "    try:\n",
    "        import boto3\n",
    "        from botocore.client import Config\n",
    "        s3 = boto3.client('s3', endpoint_url=MINIO_URL,\n",
    "                          aws_access_key_id='admin', aws_secret_access_key='password123',\n",
    "                          config=Config(signature_version='s3v4'))\n",
    "        buckets = s3.list_buckets()\n",
    "        results[\"MinIO\"] = f\"OK ({len(buckets['Buckets'])} buckets)\"\n",
    "    except Exception as e:\n",
    "        results[\"MinIO\"] = f\"FAILED: {str(e)[:40]}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Service Health Check:\")\n",
    "print(\"=\" * 50)\n",
    "for service, status in check_services().items():\n",
    "    icon = \"✓\" if \"OK\" in status else \"⚠\" if \"Not running\" in status else \"✗\"\n",
    "    print(f\"  {icon} {service}: {status}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "EXPERIMENT_NAME = \"genai-demo\"\n",
    "experiment = mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"Experiment: {experiment.name} (ID: {experiment.experiment_id})\")\n",
    "print(f\"View: http://localhost:15000/#/experiments/{experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: Via LiteLLM Proxy\n",
    "\n",
    "Uses LiteLLM as a unified proxy to access multiple LLM providers.\n",
    "\n",
    "**Requires**: `docker compose -f docker-compose.litellm.yml up -d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LiteLLM Helper Function\n",
    "# ============================================================================\n",
    "DEFAULT_MODEL = \"llama-3.1-8b\"  # Groq free tier\n",
    "\n",
    "def call_llm(prompt: str, model: str = DEFAULT_MODEL, temperature: float = 0.7, max_tokens: int = 500) -> Dict[str, Any]:\n",
    "    \"\"\"Call LiteLLM proxy.\"\"\"\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{LITELLM_URL}/chat/completions\",\n",
    "            headers={\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {LITELLM_API_KEY}\"},\n",
    "            json={\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                  \"temperature\": temperature, \"max_tokens\": max_tokens},\n",
    "            timeout=60\n",
    "        )\n",
    "        latency = time.time() - start\n",
    "        if resp.status_code != 200:\n",
    "            return {\"content\": f\"Error: HTTP {resp.status_code}\", \"model\": model, \"usage\": {}, \"latency\": latency}\n",
    "        data = resp.json()\n",
    "        if \"error\" in data:\n",
    "            return {\"content\": f\"Error: {data['error'].get('message', 'Unknown')}\", \"model\": model, \"usage\": {}, \"latency\": latency}\n",
    "        return {\n",
    "            \"content\": data[\"choices\"][0][\"message\"][\"content\"],\n",
    "            \"model\": data.get(\"model\", model),\n",
    "            \"usage\": data.get(\"usage\", {}),\n",
    "            \"latency\": latency\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"content\": f\"Error: {str(e)[:80]}\", \"model\": model, \"usage\": {}, \"latency\": time.time() - start}\n",
    "\n",
    "# Test\n",
    "print(f\"Testing LiteLLM ({DEFAULT_MODEL})...\")\n",
    "test = call_llm(\"Say 'hello'\", max_tokens=10)\n",
    "if test[\"content\"].startswith(\"Error\"):\n",
    "    print(f\"⚠ LiteLLM not available: {test['content'][:50]}\")\n",
    "    print(\"  Skip to Part 2 for standalone MLflow usage\")\n",
    "else:\n",
    "    print(f\"✓ SUCCESS: {test['content']} ({test['latency']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LLM call via LiteLLM\n",
    "response = call_llm(\"What is MLflow? Answer in 1 sentence.\")\n",
    "print(f\"Response: {response['content']}\")\n",
    "print(f\"Model: {response['model']}, Latency: {response['latency']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow tracing with decorator\n",
    "@mlflow.trace\n",
    "def analyze_text_litellm(text: str) -> Dict[str, Any]:\n",
    "    prompt = f\"Analyze sentiment of: {text}. Answer: positive/negative/neutral\"\n",
    "    return call_llm(prompt, temperature=0.3)\n",
    "\n",
    "result = analyze_text_litellm(\"The fraud detection system saved us millions!\")\n",
    "print(f\"Analysis: {result['content']}\")\n",
    "print(\"→ View trace in MLflow UI → Traces tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step pipeline with spans\n",
    "@mlflow.trace(name=\"fraud_pipeline_litellm\")\n",
    "def fraud_pipeline(txn: Dict) -> Dict:\n",
    "    results = {}\n",
    "    with mlflow.start_span(name=\"classify\") as span:\n",
    "        span.set_inputs(txn)\n",
    "        resp = call_llm(f\"Fraud risk for ${txn['amount']} at {txn['merchant']}? LOW/MEDIUM/HIGH\", temperature=0.2)\n",
    "        results[\"risk\"] = resp[\"content\"]\n",
    "        span.set_outputs({\"risk\": resp[\"content\"]})\n",
    "    with mlflow.start_span(name=\"explain\") as span:\n",
    "        resp = call_llm(f\"Why is this {results['risk']}? 1 sentence.\", temperature=0.5)\n",
    "        results[\"reason\"] = resp[\"content\"]\n",
    "        span.set_outputs({\"reason\": resp[\"content\"]})\n",
    "    return results\n",
    "\n",
    "result = fraud_pipeline({\"amount\": 5000, \"merchant\": \"Overseas Wire\"})\n",
    "print(f\"Risk: {result['risk']}\")\n",
    "print(f\"Reason: {result['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: MLflow Standalone (Direct SDK)\n",
    "\n",
    "Direct integration with Groq/OpenAI SDK - **NO LiteLLM required**.\n",
    "\n",
    "This demonstrates MLflow's native GenAI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Install OpenAI SDK (compatible with Groq)\n",
    "# ============================================================================\n",
    "# !pip install openai>=1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Setup Groq client (OpenAI-compatible)\n",
    "# ============================================================================\n",
    "from openai import OpenAI\n",
    "\n",
    "# Groq uses OpenAI-compatible API\n",
    "groq_client = OpenAI(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# Test direct Groq connection\n",
    "print(\"Testing direct Groq SDK connection...\")\n",
    "try:\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'hello'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(f\"✓ SUCCESS: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MLflow OpenAI Autolog\n",
    "\n",
    "Automatically trace all OpenAI-compatible API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Enable MLflow OpenAI Autolog\n# ============================================================================\n# Note: mlflow.openai.autolog() has minimal parameters\n# It automatically traces all OpenAI-compatible API calls\n\nmlflow.openai.autolog()\nprint(\"✓ MLflow OpenAI autolog enabled\")\nprint(\"  All OpenAI/Groq calls will be automatically traced\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Basic call with autolog (automatically traced)\n",
    "# ============================================================================\n",
    "print(\"Making LLM call with autolog...\")\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a fraud analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the top 3 signs of fraudulent transactions?\"}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "print(f\"\\n→ This call was AUTO-LOGGED to MLflow!\")\n",
    "print(f\"→ Check: http://localhost:15000 → Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Manual Tracing with @mlflow.trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Custom traced function (standalone)\n",
    "# ============================================================================\n",
    "@mlflow.trace(name=\"standalone_fraud_analysis\")\n",
    "def analyze_fraud_standalone(transaction: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Fraud analysis using direct Groq SDK with MLflow tracing.\"\"\"\n",
    "    \n",
    "    # Step 1: Risk assessment\n",
    "    with mlflow.start_span(name=\"risk_assessment\") as span:\n",
    "        span.set_inputs({\"transaction\": transaction})\n",
    "        \n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Assess fraud risk:\n",
    "Amount: ${transaction['amount']}\n",
    "Merchant: {transaction['merchant']}\n",
    "Time: {transaction['time']}\n",
    "\n",
    "Respond with: RISK_LEVEL (LOW/MEDIUM/HIGH) and confidence (0-100)\"\"\"\n",
    "            }],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        risk = response.choices[0].message.content\n",
    "        span.set_outputs({\"risk\": risk, \"tokens\": response.usage.total_tokens})\n",
    "    \n",
    "    # Step 2: Generate explanation\n",
    "    with mlflow.start_span(name=\"explanation\") as span:\n",
    "        span.set_inputs({\"risk\": risk})\n",
    "        \n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Explain in 1 sentence why this transaction is {risk}\"\n",
    "            }],\n",
    "            max_tokens=100,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        explanation = response.choices[0].message.content\n",
    "        span.set_outputs({\"explanation\": explanation})\n",
    "    \n",
    "    return {\"risk\": risk, \"explanation\": explanation}\n",
    "\n",
    "# Test\n",
    "result = analyze_fraud_standalone({\n",
    "    \"amount\": 3500,\n",
    "    \"merchant\": \"Online Casino\",\n",
    "    \"time\": \"2:30 AM\"\n",
    "})\n",
    "\n",
    "print(\"Standalone Fraud Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Risk: {result['risk']}\")\n",
    "print(f\"Explanation: {result['explanation']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"→ View hierarchical trace in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Manual Logging with MLflow Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Full manual control over logging\n",
    "# ============================================================================\n",
    "def call_groq_with_logging(prompt: str, model: str = \"llama-3.1-8b-instant\", run_name: str = None):\n",
    "    \"\"\"Call Groq with full manual MLflow logging.\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name or f\"groq_{model}\"):\n",
    "        # Log input parameters\n",
    "        mlflow.log_param(\"model\", model)\n",
    "        mlflow.log_param(\"prompt_length\", len(prompt))\n",
    "        mlflow.log_param(\"prompt_preview\", prompt[:100])\n",
    "        \n",
    "        # Make API call\n",
    "        start_time = time.time()\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"latency_seconds\", latency)\n",
    "        mlflow.log_metric(\"prompt_tokens\", response.usage.prompt_tokens)\n",
    "        mlflow.log_metric(\"completion_tokens\", response.usage.completion_tokens)\n",
    "        mlflow.log_metric(\"total_tokens\", response.usage.total_tokens)\n",
    "        mlflow.log_metric(\"response_length\", len(response.choices[0].message.content))\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_text(prompt, \"prompt.txt\")\n",
    "        mlflow.log_text(response.choices[0].message.content, \"response.txt\")\n",
    "        \n",
    "        # Log as JSON for structured data\n",
    "        mlflow.log_dict({\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens,\n",
    "                \"total_tokens\": response.usage.total_tokens\n",
    "            },\n",
    "            \"latency\": latency\n",
    "        }, \"call_details.json\")\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Test manual logging\n",
    "print(\"Testing manual MLflow logging...\")\n",
    "result = call_groq_with_logging(\n",
    "    \"What is feature drift and why does it matter for fraud detection?\",\n",
    "    run_name=\"manual_logging_demo\"\n",
    ")\n",
    "print(f\"\\nResponse: {result[:200]}...\")\n",
    "print(\"\\n→ Check MLflow UI → genai-demo experiment → manual_logging_demo run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model Comparison (Standalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compare models using direct SDK\n",
    "# ============================================================================\n",
    "def compare_models_standalone(prompt: str, models: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Compare multiple Groq models.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        with mlflow.start_run(run_name=f\"standalone_compare_{model.split('-')[0]}\"):\n",
    "            mlflow.log_param(\"model\", model)\n",
    "            mlflow.log_param(\"method\", \"direct_groq_sdk\")\n",
    "            \n",
    "            start = time.time()\n",
    "            try:\n",
    "                response = groq_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                latency = time.time() - start\n",
    "                content = response.choices[0].message.content\n",
    "                tokens = response.usage.total_tokens\n",
    "                \n",
    "                mlflow.log_metric(\"latency\", latency)\n",
    "                mlflow.log_metric(\"tokens\", tokens)\n",
    "                mlflow.log_text(content, \"response.txt\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"model\": model,\n",
    "                    \"status\": \"✓\",\n",
    "                    \"latency\": f\"{latency:.2f}s\",\n",
    "                    \"tokens\": tokens,\n",
    "                    \"preview\": content[:50] + \"...\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"model\": model,\n",
    "                    \"status\": \"✗\",\n",
    "                    \"latency\": \"-\",\n",
    "                    \"tokens\": 0,\n",
    "                    \"preview\": str(e)[:50]\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare Groq models\n",
    "print(\"Comparing Groq models (standalone)...\\n\")\n",
    "df = compare_models_standalone(\n",
    "    \"Explain overfitting in 2 sentences.\",\n",
    "    [\"llama-3.1-8b-instant\", \"llama-3.3-70b-versatile\"]\n",
    ")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n→ Compare runs in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# PART 3: Advanced MLflow GenAI Features\n\nThis section demonstrates:\n1. **Prompt Registry** - Version and track prompts in MLflow UI\n2. **Model Evaluation** - `mlflow.genai.evaluate()` with scorers\n3. **A/B Testing** - Compare models with evaluation metrics\n\n**MLflow UI Prompts Tab**: View all registered prompts, compare versions side-by-side with diff highlighting.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 Prompt Registry - Version Control for Prompts\n\nThe MLflow Prompt Registry provides:\n- **Version Control**: Git-inspired commit-based versioning\n- **Aliasing**: Flexible deployment (production, staging, etc.)\n- **Lineage**: Track which prompts were used in which runs\n- **Collaboration**: Share prompts across your organization\n\n**In MLflow UI**: Navigate to **Prompts** tab to see all registered prompts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# 3.1 Check for MLflow GenAI Module\n# ============================================================================\n# Check if mlflow.genai is available (MLflow 2.10+)\ntry:\n    import mlflow.genai\n    HAS_GENAI = True\n    print(f\"✓ mlflow.genai module available (MLflow {mlflow.__version__})\")\nexcept ImportError:\n    HAS_GENAI = False\n    print(f\"⚠ mlflow.genai not available (MLflow {mlflow.__version__})\")\n    print(\"  Requires MLflow 2.10+ for Prompt Registry\")\n    print(\"  Will use manual prompt tracking instead\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"  MLflow GenAI Demo Complete!\")\nprint(\"=\" * 70)\nprint(f\"\"\"\n  PART 1: Via LiteLLM Proxy\n    - Auto-logging via success_callback\n    - @mlflow.trace decorator\n    - Custom spans with start_span()\n    \n  PART 2: MLflow Standalone (Direct SDK)\n    - mlflow.openai.autolog() - automatic tracing\n    - Direct Groq SDK with manual tracing\n    - Full control with log_param/log_metric/log_artifact\n    \n  PART 3: Advanced GenAI Features (below)\n    - Prompt Registry - version control for prompts\n    - Model Evaluation - mlflow.genai.evaluate() with scorers\n    - A/B Testing - compare models and prompts\n    \n  MLflow UI: http://localhost:15000\n  Experiment: {EXPERIMENT_NAME}\n  \n  UI Tabs:\n  → Experiments: View all runs, compare metrics\n  → Traces: View LLM call hierarchies\n  → Prompts: View registered prompts, compare versions\n  → Evaluation: View evaluation results\n\"\"\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Register Fraud Analysis Prompts (Multiple Versions)\n# ============================================================================\nif HAS_GENAI:\n    # Version 1: Simple prompt\n    try:\n        prompt_v1 = mlflow.genai.register_prompt(\n            name=\"fraud-analysis-prompt\",\n            template=\"\"\"Analyze this transaction for fraud:\nAmount: {{ amount }}\nMerchant: {{ merchant }}\n\nIs this fraudulent? Answer YES or NO with brief reason.\"\"\",\n            commit_message=\"v1: Simple fraud detection prompt\"\n        )\n        print(f\"✓ Registered prompt v1: {prompt_v1.name}\")\n    except Exception as e:\n        print(f\"  Prompt may already exist: {str(e)[:50]}\")\n    \n    # Version 2: Enhanced prompt with more context\n    try:\n        prompt_v2 = mlflow.genai.register_prompt(\n            name=\"fraud-analysis-prompt\",\n            template=\"\"\"You are a fraud detection expert. Analyze this transaction:\n\nTransaction Details:\n- Amount: ${{ amount }}\n- Merchant: {{ merchant }}\n- Time: {{ time }}\n- Location: {{ location }}\n\nConsider:\n1. Is the amount unusual?\n2. Is the merchant suspicious?\n3. Is the timing suspicious?\n\nVerdict: [FRAUD/LEGITIMATE]\nConfidence: [0-100]%\nReasoning: [Brief explanation]\"\"\",\n            commit_message=\"v2: Enhanced prompt with structured output\"\n        )\n        print(f\"✓ Registered prompt v2: {prompt_v2.name}\")\n    except Exception as e:\n        print(f\"  v2: {str(e)[:50]}\")\n    \n    print(\"\\n→ View prompts in MLflow UI → Prompts tab\")\n    print(\"→ Compare versions with diff highlighting\")\nelse:\n    print(\"Skipping prompt registry (mlflow.genai not available)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Load and Use Registered Prompts\n# ============================================================================\nif HAS_GENAI:\n    # Load specific version using URI format: prompts:/<name>/<version>\n    try:\n        # Load latest version\n        prompt = mlflow.genai.load_prompt(\"prompts:/fraud-analysis-prompt/1\")\n        print(f\"Loaded prompt: {prompt.name}\")\n        \n        # Format with variables\n        formatted = prompt.format(\n            amount=\"5000\",\n            merchant=\"Overseas Wire Transfer\"\n        )\n        print(f\"\\nFormatted prompt:\\n{'-'*50}\\n{formatted}\\n{'-'*50}\")\n        \n        # Use with LLM\n        response = groq_client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=[{\"role\": \"user\", \"content\": formatted}],\n            max_tokens=100\n        )\n        print(f\"\\nLLM Response: {response.choices[0].message.content}\")\n        \n    except Exception as e:\n        print(f\"Could not load prompt: {e}\")\n        print(\"This may require MLflow 2.15+ with Prompt Registry enabled\")\nelse:\n    # Fallback: Manual prompt tracking\n    print(\"Using manual prompt tracking (no mlflow.genai)\")\n    \n    PROMPTS = {\n        \"v1\": \"Analyze this transaction for fraud: Amount: {amount}, Merchant: {merchant}. Fraudulent? YES/NO\",\n        \"v2\": \"You are a fraud expert. Transaction: ${amount} at {merchant}. Verdict: FRAUD/LEGITIMATE, Confidence: %, Reason:\"\n    }\n    \n    for version, template in PROMPTS.items():\n        formatted = template.format(amount=\"5000\", merchant=\"Overseas Wire\")\n        print(f\"\\n{version}: {formatted[:80]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Model Evaluation with Scorers\n\nMLflow GenAI provides `mlflow.genai.evaluate()` for systematic LLM evaluation:\n\n**Built-in Scorers:**\n- `Correctness()` - Compares output against expected facts/answers\n- `RelevanceToQuery()` - Measures response relevance\n- `Guidelines()` - Checks compliance with custom criteria\n- `Safety()` - Detects harmful content\n\n**Custom Scorers:** Write your own with the `@scorer` decorator.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# 3.2 Model Evaluation - Create Evaluation Dataset\n# ============================================================================\n# Evaluation dataset with inputs, expected outputs, and ground truth\neval_dataset = [\n    {\n        \"inputs\": {\"query\": \"Is a $50 purchase at Starbucks fraudulent?\"},\n        \"expectations\": {\"expected_response\": \"No, this is a normal low-value retail transaction.\"}\n    },\n    {\n        \"inputs\": {\"query\": \"Is a $5000 wire transfer to Nigeria at 3AM fraudulent?\"},\n        \"expectations\": {\"expected_response\": \"Yes, this shows multiple fraud indicators: high amount, international wire, unusual time.\"}\n    },\n    {\n        \"inputs\": {\"query\": \"Is a $200 Amazon purchase fraudulent?\"},\n        \"expectations\": {\"expected_response\": \"No, this is a typical e-commerce transaction.\"}\n    },\n    {\n        \"inputs\": {\"query\": \"Is a $10000 casino transaction with new card fraudulent?\"},\n        \"expectations\": {\"expected_response\": \"Yes, high-risk merchant with large amount on new card.\"}\n    },\n]\n\nprint(f\"Evaluation dataset: {len(eval_dataset)} test cases\")\nfor i, item in enumerate(eval_dataset):\n    print(f\"  {i+1}. {item['inputs']['query'][:50]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Define Predict Function and Custom Scorers\n# ============================================================================\n\n# Predict function - must match input key names\ndef fraud_predict_fn(query: str) -> str:\n    \"\"\"Predict function for evaluation. Parameter name must match 'inputs' keys.\"\"\"\n    response = groq_client.chat.completions.create(\n        model=\"llama-3.1-8b-instant\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a fraud analyst. Give brief, direct answers.\"},\n            {\"role\": \"user\", \"content\": query}\n        ],\n        max_tokens=100,\n        temperature=0.2\n    )\n    return response.choices[0].message.content\n\n# Test predict function\ntest_output = fraud_predict_fn(\"Is a $50 Starbucks purchase fraudulent?\")\nprint(f\"Test prediction: {test_output[:100]}...\")\n\n# Custom scorers (code-based)\ndef is_concise(inputs: Dict, outputs: str, expectations: Dict) -> bool:\n    \"\"\"Check if response is concise (under 50 words).\"\"\"\n    return len(outputs.split()) <= 50\n\ndef has_verdict(inputs: Dict, outputs: str, expectations: Dict) -> bool:\n    \"\"\"Check if response contains a clear verdict.\"\"\"\n    lower = outputs.lower()\n    return any(word in lower for word in [\"yes\", \"no\", \"fraud\", \"legitimate\", \"suspicious\", \"normal\"])\n\ndef response_length(inputs: Dict, outputs: str, expectations: Dict) -> int:\n    \"\"\"Return response length in words.\"\"\"\n    return len(outputs.split())\n\nprint(\"\\n✓ Custom scorers defined: is_concise, has_verdict, response_length\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Run Evaluation with mlflow.genai.evaluate()\n# ============================================================================\nif HAS_GENAI:\n    try:\n        # Import scorers\n        from mlflow.genai.scorers import Correctness, RelevanceToQuery, Guidelines\n        \n        # Run evaluation\n        print(\"Running MLflow GenAI evaluation...\")\n        results = mlflow.genai.evaluate(\n            data=eval_dataset,\n            predict_fn=fraud_predict_fn,\n            scorers=[\n                Correctness(),           # Compare to expected_response\n                RelevanceToQuery(),      # Check relevance to query\n                is_concise,              # Custom: under 50 words\n                has_verdict,             # Custom: contains clear verdict\n                response_length,         # Custom: word count\n            ]\n        )\n        \n        print(\"\\n✓ Evaluation complete!\")\n        print(f\"→ View detailed results in MLflow UI → Evaluation tab\")\n        \n        # Display summary\n        if hasattr(results, 'tables') and 'eval_results' in results.tables:\n            print(\"\\nResults Preview:\")\n            print(results.tables['eval_results'].head())\n            \n    except ImportError as e:\n        print(f\"⚠ Scorers not available: {e}\")\n        print(\"  Using manual evaluation instead...\")\n    except Exception as e:\n        print(f\"⚠ Evaluation error: {e}\")\n        print(\"  This may require MLflow 2.15+ or specific configuration\")\nelse:\n    print(\"Skipping mlflow.genai.evaluate() - module not available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Manual Evaluation (Works with any MLflow version)\n# ============================================================================\nprint(\"Running manual evaluation with MLflow tracking...\\n\")\n\neval_results = []\n\nwith mlflow.start_run(run_name=\"fraud_model_evaluation\"):\n    mlflow.log_param(\"model\", \"llama-3.1-8b-instant\")\n    mlflow.log_param(\"evaluation_size\", len(eval_dataset))\n    mlflow.log_param(\"method\", \"manual_eval\")\n    \n    for i, item in enumerate(eval_dataset):\n        query = item[\"inputs\"][\"query\"]\n        expected = item[\"expectations\"][\"expected_response\"]\n        \n        # Get prediction\n        output = fraud_predict_fn(query)\n        \n        # Score with custom metrics\n        concise = len(output.split()) <= 50\n        has_clear_verdict = any(w in output.lower() for w in [\"yes\", \"no\", \"fraud\", \"legitimate\"])\n        word_count = len(output.split())\n        \n        # Simple correctness check (keyword match)\n        expected_lower = expected.lower()\n        output_lower = output.lower()\n        if \"yes\" in expected_lower or \"fraud\" in expected_lower:\n            correct = any(w in output_lower for w in [\"yes\", \"fraud\", \"suspicious\", \"high risk\"])\n        else:\n            correct = any(w in output_lower for w in [\"no\", \"legitimate\", \"normal\", \"typical\"])\n        \n        eval_results.append({\n            \"query\": query[:40] + \"...\",\n            \"correct\": \"✓\" if correct else \"✗\",\n            \"concise\": \"✓\" if concise else \"✗\",\n            \"verdict\": \"✓\" if has_clear_verdict else \"✗\",\n            \"words\": word_count\n        })\n        \n        print(f\"  [{i+1}] {'✓' if correct else '✗'} {query[:50]}...\")\n    \n    # Calculate aggregate metrics\n    correct_count = sum(1 for r in eval_results if r[\"correct\"] == \"✓\")\n    concise_count = sum(1 for r in eval_results if r[\"concise\"] == \"✓\")\n    verdict_count = sum(1 for r in eval_results if r[\"verdict\"] == \"✓\")\n    avg_words = sum(r[\"words\"] for r in eval_results) / len(eval_results)\n    \n    # Log metrics\n    mlflow.log_metric(\"accuracy\", correct_count / len(eval_dataset))\n    mlflow.log_metric(\"conciseness_rate\", concise_count / len(eval_dataset))\n    mlflow.log_metric(\"verdict_rate\", verdict_count / len(eval_dataset))\n    mlflow.log_metric(\"avg_response_words\", avg_words)\n    \n    # Log results as artifact\n    mlflow.log_dict({\"results\": eval_results}, \"eval_results.json\")\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Evaluation Summary:\")\n    print(f\"  Accuracy:    {correct_count}/{len(eval_dataset)} ({100*correct_count/len(eval_dataset):.0f}%)\")\n    print(f\"  Concise:     {concise_count}/{len(eval_dataset)}\")\n    print(f\"  Has Verdict: {verdict_count}/{len(eval_dataset)}\")\n    print(f\"  Avg Words:   {avg_words:.1f}\")\n    print(f\"{'='*50}\")\n    print(f\"\\n→ View run in MLflow UI: fraud_model_evaluation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3 A/B Testing - Compare Models with Evaluation Metrics\n\nCompare multiple models on the same evaluation dataset to determine which performs best for your use case.\n\n**What to compare:**\n- Different models (llama-3.1-8b vs llama-3.3-70b)\n- Different prompts (v1 vs v2)\n- Different temperatures\n- Different system prompts",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# 3.3 A/B Testing - Compare Two Models\n# ============================================================================\n\n# Models to compare (Groq free tier)\nMODELS_TO_TEST = [\n    {\"name\": \"llama-3.1-8b-instant\", \"alias\": \"Model A (Fast)\"},\n    {\"name\": \"llama-3.3-70b-versatile\", \"alias\": \"Model B (Large)\"},\n]\n\n# Prompts to test\nPROMPTS_TO_TEST = [\n    {\n        \"name\": \"v1_simple\",\n        \"system\": \"You are a fraud analyst.\",\n        \"template\": \"Is this fraudulent? {query} Answer YES or NO briefly.\"\n    },\n    {\n        \"name\": \"v2_detailed\",\n        \"system\": \"You are an expert fraud detection analyst. Be thorough but concise.\",\n        \"template\": \"Analyze for fraud: {query}\\nProvide: Verdict (FRAUD/LEGITIMATE), Confidence (%), Brief reason.\"\n    },\n]\n\nprint(f\"A/B Test Configuration:\")\nprint(f\"  Models: {[m['alias'] for m in MODELS_TO_TEST]}\")\nprint(f\"  Prompts: {[p['name'] for p in PROMPTS_TO_TEST]}\")\nprint(f\"  Test cases: {len(eval_dataset)}\")\nprint(f\"  Total runs: {len(MODELS_TO_TEST) * len(PROMPTS_TO_TEST)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Run A/B Test - Evaluate Each Model/Prompt Combination\n# ============================================================================\nab_results = []\n\nprint(\"Running A/B Tests...\\n\")\n\nfor model_config in MODELS_TO_TEST:\n    for prompt_config in PROMPTS_TO_TEST:\n        run_name = f\"ab_{model_config['name'].split('-')[0]}_{prompt_config['name']}\"\n        \n        print(f\"Testing: {model_config['alias']} + {prompt_config['name']}...\")\n        \n        with mlflow.start_run(run_name=run_name):\n            # Log configuration\n            mlflow.log_param(\"model\", model_config[\"name\"])\n            mlflow.log_param(\"model_alias\", model_config[\"alias\"])\n            mlflow.log_param(\"prompt_version\", prompt_config[\"name\"])\n            mlflow.log_param(\"system_prompt\", prompt_config[\"system\"][:50])\n            mlflow.log_param(\"test_type\", \"ab_test\")\n            \n            # Run evaluation\n            correct = 0\n            total_latency = 0\n            total_tokens = 0\n            \n            for item in eval_dataset:\n                query = item[\"inputs\"][\"query\"]\n                expected = item[\"expectations\"][\"expected_response\"]\n                \n                # Format prompt\n                formatted_query = prompt_config[\"template\"].format(query=query)\n                \n                # Call model\n                start = time.time()\n                try:\n                    response = groq_client.chat.completions.create(\n                        model=model_config[\"name\"],\n                        messages=[\n                            {\"role\": \"system\", \"content\": prompt_config[\"system\"]},\n                            {\"role\": \"user\", \"content\": formatted_query}\n                        ],\n                        max_tokens=100,\n                        temperature=0.2\n                    )\n                    latency = time.time() - start\n                    output = response.choices[0].message.content\n                    tokens = response.usage.total_tokens\n                    \n                    total_latency += latency\n                    total_tokens += tokens\n                    \n                    # Check correctness\n                    expected_lower = expected.lower()\n                    output_lower = output.lower()\n                    if \"yes\" in expected_lower or \"fraud\" in expected_lower:\n                        if any(w in output_lower for w in [\"yes\", \"fraud\", \"suspicious\", \"high\"]):\n                            correct += 1\n                    else:\n                        if any(w in output_lower for w in [\"no\", \"legitimate\", \"normal\", \"low\"]):\n                            correct += 1\n                            \n                except Exception as e:\n                    print(f\"    Error: {str(e)[:30]}\")\n                    latency = time.time() - start\n                    total_latency += latency\n            \n            # Calculate and log metrics\n            accuracy = correct / len(eval_dataset)\n            avg_latency = total_latency / len(eval_dataset)\n            avg_tokens = total_tokens / len(eval_dataset)\n            \n            mlflow.log_metric(\"accuracy\", accuracy)\n            mlflow.log_metric(\"avg_latency\", avg_latency)\n            mlflow.log_metric(\"avg_tokens\", avg_tokens)\n            mlflow.log_metric(\"total_correct\", correct)\n            \n            ab_results.append({\n                \"model\": model_config[\"alias\"],\n                \"prompt\": prompt_config[\"name\"],\n                \"accuracy\": f\"{100*accuracy:.0f}%\",\n                \"latency\": f\"{avg_latency:.2f}s\",\n                \"tokens\": f\"{avg_tokens:.0f}\",\n                \"run_name\": run_name\n            })\n            \n            print(f\"  → Accuracy: {100*accuracy:.0f}%, Latency: {avg_latency:.2f}s\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# A/B Test Results Summary\n# ============================================================================\nprint(\"A/B TEST RESULTS\")\nprint(\"=\"*60)\n\ndf_ab = pd.DataFrame(ab_results)\nprint(df_ab.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"WINNER ANALYSIS:\")\n\n# Find best by accuracy\nbest_accuracy = df_ab.loc[df_ab['accuracy'].str.rstrip('%').astype(int).idxmax()]\nprint(f\"  Best Accuracy: {best_accuracy['model']} + {best_accuracy['prompt']} ({best_accuracy['accuracy']})\")\n\n# Find fastest\nbest_latency = df_ab.loc[df_ab['latency'].str.rstrip('s').astype(float).idxmin()]\nprint(f\"  Fastest:       {best_latency['model']} + {best_latency['prompt']} ({best_latency['latency']})\")\n\n# Find most efficient (tokens)\nbest_tokens = df_ab.loc[df_ab['tokens'].astype(int).idxmin()]\nprint(f\"  Most Efficient: {best_tokens['model']} + {best_tokens['prompt']} ({best_tokens['tokens']} tokens)\")\n\nprint(\"=\"*60)\nprint(f\"\"\"\n→ Compare runs in MLflow UI:\n  1. Go to http://localhost:15000\n  2. Select 'genai-demo' experiment\n  3. Select A/B test runs (filter by test_type=ab_test)\n  4. Click 'Compare' to see side-by-side metrics\n  5. Use Charts to visualize accuracy vs latency tradeoffs\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# View all runs\n",
    "# ============================================================================\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "if exp:\n",
    "    runs = client.search_runs([exp.experiment_id], max_results=15, order_by=[\"start_time DESC\"])\n",
    "    print(f\"Recent Runs in '{EXPERIMENT_NAME}':\")\n",
    "    print(\"=\" * 70)\n",
    "    for run in runs:\n",
    "        method = run.data.params.get(\"method\", \"litellm\")\n",
    "        metrics = \", \".join([f\"{k}={v:.2f}\" for k,v in list(run.data.metrics.items())[:2]]) or \"no metrics\"\n",
    "        print(f\"  [{method[:8]:8}] {run.info.run_name[:25]:25} | {metrics}\")\n",
    "    print(f\"\\n→ View all: http://localhost:15000/#/experiments/{exp.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  MLflow GenAI Demo Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "  PART 1: Via LiteLLM Proxy\n",
    "    - Auto-logging via success_callback\n",
    "    - @mlflow.trace decorator\n",
    "    - Custom spans with start_span()\n",
    "    \n",
    "  PART 2: MLflow Standalone (Direct SDK)\n",
    "    - mlflow.openai.autolog() - automatic tracing\n",
    "    - Direct Groq SDK with manual tracing\n",
    "    - Full control with log_param/log_metric/log_artifact\n",
    "    \n",
    "  MLflow UI: http://localhost:15000\n",
    "  Experiment: {EXPERIMENT_NAME}\n",
    "  → Traces tab: View LLM call hierarchies\n",
    "  → Runs: Compare model/prompt performance\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}